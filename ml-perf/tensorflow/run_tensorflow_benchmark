#!/bin/sh
[ -f "./configure" ] && . "./configure"
[ -f "../configure" ] && . "../configure"

pre_clean ()
{
    # check whether temp result file exists
    if [ -e "${RESULTDIR}/tmp.txt" ]; then
        rm "${RESULTDIR}/tmp.txt"
    fi
    if [ -e "${RESULTDIR}/bench_report.txt" ]; then
        rm "${RESULTDIR}/bench_report.txt"
    fi
}

post_clean ()
{
    # remove the intermediate result files
    if [ -e "${RESULTDIR}/tmp.txt" ]; then
        rm "${RESULTDIR}/tmp.txt"
    fi
    if [ -e "${RESULTDIR}/bench_report.txt" ]; then
        rm "${RESULTDIR}/bench_report.txt"
    fi
    if [ -e "${RESULTDIR}/side_band_info.txt" ]; then
        rm "${RESULTDIR}/side_band_info.txt"
    fi

}

run_monoshot ()
{
    pre_clean
    time=${TIME_NOW}
    ${UTILDIR}/cpu_load 1 90 ${RESULTDIR}/tf/cpu_load_${network}_${benchmark_type}.${time} &
    pid=$!
    ${TENSORFLOW_BENCH_DIR}/benchmark_model \
        --graph=${MODELDIR}/tf/${model_file} \
        --input_layer=${input_name} \
        --input_layer_shape=${input_shape} \
        --input_layer_type=${input_type} \
        --show_memory=false \
        --show_type=false \
        --show_time=false \
        --show_flops=true \
        --num_threads=${thread_count_default} \
        --output_layer=${output_name} 2>&1 | tee ${RESULTDIR}/tmp.txt
    wait
    cpu_util=$(${UTILDIR}/parse_cpu_load   ${RESULTDIR}/tf/cpu_load_${network}_${benchmark_type}.${time})
    echo "cpu utilization = ${cpu_util}" >> ${RESULTDIR}/side_band_info.txt
    rm ${RESULTDIR}/tf/cpu_load_${network}_${benchmark_type}.${time}
    # update the final result file
    ${ANALYZEDIR}/tf/tensorflow_analyze_results \
        ${network} \
        ${benchmark_type} \
        ${RESULTDIR}/tmp.txt \
        ${RESULTDIR}/bench_report.txt
        
    cat ${RESULTDIR}/side_band_info.txt >> ${RESULTDIR}/tf/${network}_${benchmark_type}.${time}
    cat ${RESULTDIR}/bench_report.txt >> ${RESULTDIR}/tf/${network}_${benchmark_type}.${time}
    post_clean
}

run_scaling_wrt_core ()
{
    pre_clean
    time=${TIME_NOW}
    for k in $(seq 1 ${TOTAL_CORES})
    do
        var="NUM_CORE_"${k}
        eval cpu_mask="\$$var"
        ${UTILDIR}/cpu_load 1 90 ${RESULTDIR}/tf/cpu_load_${network}_${benchmark_type}.${time} &
        pid=$!
        ${TASKSET} ${cpu_mask} ${TENSORFLOW_BENCH_DIR}/benchmark_model \
            --graph=${MODELDIR}/tf/${model_file} \
            --input_layer=${input_name} \
            --input_layer_shape=${input_shape} \
            --input_layer_type=${input_type} \
            --show_memory=false \
            --show_type=false \
            --show_time=false \
            --show_flops=true \
            --num_threads=${thread_count_default} \
            --output_layer=${output_name} 2>&1 | tee -a ${RESULTDIR}/tmp.txt
        if [ "$?" != "0" ]; then
            echo "FAILED"
            return
        fi
        wait
        cpu_util=$(${UTILDIR}/parse_cpu_load   ${RESULTDIR}/tf/cpu_load_${network}_${benchmark_type}.${time})
        echo "#core = ${k} cpu utilization = ${cpu_util}" >> ${RESULTDIR}/side_band_info.txt
        rm ${RESULTDIR}/tf/cpu_load_${network}_${benchmark_type}.${time}
    done
    # update the final result file
    ${ANALYZEDIR}/tf/tensorflow_analyze_results \
        ${network} \
        ${benchmark_type} \
        ${RESULTDIR}/tmp.txt \
        ${RESULTDIR}/bench_report.txt
        
    cat ${RESULTDIR}/side_band_info.txt >> ${RESULTDIR}/tf/${network}_${benchmark_type}.${time}
    cat ${RESULTDIR}/bench_report.txt >> ${RESULTDIR}/tf/${network}_${benchmark_type}.${time}
    post_clean
}

run_scaling_wrt_core_thread ()
{
    pre_clean
    time=${TIME_NOW}
    for k in $(seq 1 ${TOTAL_CORES})
    do
        var="NUM_CORE_"${k}
        eval cpu_mask="\$$var"
        for j in $(seq 1 ${thread})
        do
            ${UTILDIR}/cpu_load 1 600 ${RESULTDIR}/tf/cpu_load_${network}_${benchmark_type}.${time} &
            pid=$!
            ${TASKSET} ${cpu_mask} ${TENSORFLOW_BENCH_DIR}/benchmark_model \
                --graph=${MODELDIR}/tf/${model_file} \
                --input_layer=${input_name} \
                --input_layer_shape=${input_shape} \
                --input_layer_type=${input_type} \
                --show_memory=false \
                --show_type=false \
                --show_time=false \
                --show_flops=true \
                --num_threads=${j} \
                --output_layer=${output_name} 2>&1 | tee -a ${RESULTDIR}/tmp.txt
                if [ "$?" != "0" ]; then
                echo "FAILED"
                return
            fi
            wait
            cpu_util=$(${UTILDIR}/parse_cpu_load   ${RESULTDIR}/tf/cpu_load_${network}_${benchmark_type}.${time})
            echo "#core = ${k} #thread = ${j} cpu utilization = ${cpu_util}" >> ${RESULTDIR}/side_band_info.txt
            rm ${RESULTDIR}/tf/cpu_load_${network}_${benchmark_type}.${time}
        done
    done
    # update the final result file
    ${ANALYZEDIR}/tf/tensorflow_analyze_results \
        ${network} \
        ${benchmark_type} \
        ${RESULTDIR}/tmp.txt \
        ${RESULTDIR}/bench_report.txt \
        ${thread}
    cat ${RESULTDIR}/side_band_info.txt >> ${RESULTDIR}/tf/${network}_${benchmark_type}.${time}
    cat ${RESULTDIR}/bench_report.txt >> ${RESULTDIR}/tf/${network}_${benchmark_type}.${time}
    post_clean
}

run_scaling_wrt_thread ()
{
    pre_clean
    time=${TIME_NOW}
    for k in $(seq 1 ${thread})
    do
        ${UTILDIR}/cpu_load 1 90 ${RESULTDIR}/tf/cpu_load_${network}_${benchmark_type}.${time} &
        pid=$!
        ${TENSORFLOW_BENCH_DIR}/benchmark_model \
            --graph=${MODELDIR}/tf/${model_file} \
            --input_layer=${input_name} \
            --input_layer_shape=${input_shape} \
            --input_layer_type=${input_type} \
            --show_memory=false \
            --show_type=false \
            --show_time=false \
            --show_flops=true \
            --num_threads=${k} \
            --output_layer=${output_name} 2>&1 | tee -a ${RESULTDIR}/tmp.txt
        if [ "$?" != "0" ]; then
            echo "FAILED"
            return
        fi
        wait
        cpu_util=$(${UTILDIR}/parse_cpu_load   ${RESULTDIR}/tf/cpu_load_${network}_${benchmark_type}.${time})
        echo "#thread = ${k} cpu utilization = ${cpu_util}" >> ${RESULTDIR}/side_band_info.txt
        rm ${RESULTDIR}/tf/cpu_load_${network}_${benchmark_type}.${time}
    done
    # update the final result file
    ${ANALYZEDIR}/tf/tensorflow_analyze_results \
        ${network} \
        ${benchmark_type} \
        ${RESULTDIR}/tmp.txt \
        ${RESULTDIR}/bench_report.txt \
        ${thread}
        
    cat ${RESULTDIR}/side_band_info.txt >> ${RESULTDIR}/tf/${network}_${benchmark_type}.${time}
    cat ${RESULTDIR}/bench_report.txt >> ${RESULTDIR}/tf/${network}_${benchmark_type}.${time}
    post_clean
}

run_scaling_wrt_batch ()
{
    # for future when batching is going to be supported
    pre_clean
    time=${TIME_NOW}
    xdim=$(echo "${input_shape}"| cut -d, -f2)
    ydim=$(echo "${input_shape}"| cut -d, -f3)
    channel=$(echo "${input_shape}"| cut -d, -f4)
    if [ "${start}" = "1" ]; then
        start="$((start-1))"
    fi
    for i in $(seq ${start} ${step} ${end})
    do
        ${UTILDIR}/cpu_load 1 90 ${RESULTDIR}/tf/cpu_load_${network}_${benchmark_type}.${time} &
        pid=$!
        if [ "${i}" = "0" ]; then
            i="$((i+1))"
        fi
        ${TENSORFLOW_BENCH_DIR}/benchmark_model \
            --graph=${MODELDIR}/tf/${model_file} \
            --input_layer=${input_name} \
            --input_layer_shape=${i},${xdim},${ydim},${channel} \
            --input_layer_type=${input_type} \
            --show_memory=false \
            --show_type=false \
            --show_time=false \
            --show_flops=true \
            --num_threads=${thread_count_default} \
            --output_layer=${output_name} 2>&1 | tee -a ${RESULTDIR}/tmp.txt
        wait
        cpu_util=$(${UTILDIR}/parse_cpu_load   ${RESULTDIR}/tf/cpu_load_${network}_${benchmark_type}.${time})
        echo "#batch = ${i} cpu utilization = ${cpu_util}" >> ${RESULTDIR}/side_band_info.txt
        rm ${RESULTDIR}/tf/cpu_load_${network}_${benchmark_type}.${time}
    done
    [ "${start}" = "0" ] && start="$((start+1))"
    # update the final result file
    ${ANALYZEDIR}/tf/tensorflow_analyze_results \
        ${network} \
        ${benchmark_type} \
        ${RESULTDIR}/tmp.txt \
        ${RESULTDIR}/bench_report.txt \
        "${start}:${end}:${step}"
    cat ${RESULTDIR}/side_band_info.txt >> ${RESULTDIR}/tf/${network}_${benchmark_type}.${time}
    cat ${RESULTDIR}/bench_report.txt >> ${RESULTDIR}/tf/${network}_${benchmark_type}.${time}
    post_clean
}

run_scaling_wrt_shape ()
{
    # for future when batching is going to be supported
    pre_clean
    time=${TIME_NOW}
    echo "xdim=${xdim}, ydim=${ydim} channel=${channel}"
    ${UTILDIR}/cpu_load 1 90 ${RESULTDIR}/tf/cpu_load_${network}_${benchmark_type}.${time} &
    pid=$!
    ${TENSORFLOW_BENCH_DIR}/benchmark_model \
        --graph=${MODELDIR}/tf/${model_file} \
        --input_layer=${input_name} \
        --input_layer_shape=${batch},${xdim},${ydim},${channel} \
        --input_layer_type=${input_type} \
        --show_memory=false \
        --show_type=false \
        --show_time=false \
        --show_flops=true \
        --num_threads=${thread_count_default} \
        --output_layer=${output_name} 2>&1 | tee -a ${RESULTDIR}/tmp.txt
    wait
    cpu_util=$(${UTILDIR}/parse_cpu_load   ${RESULTDIR}/tf/cpu_load_${network}_${benchmark_type}.${time})
    echo "shape = ${xdim}:${ydim}:${channel} cpu utilization = ${cpu_util}" >> ${RESULTDIR}/side_band_info.txt
    rm ${RESULTDIR}/tf/cpu_load_${network}_${benchmark_type}.${time}
    # update the final result file
    ${ANALYZEDIR}/tf/tensorflow_analyze_results \
        ${network} \
        ${benchmark_type} \
        ${RESULTDIR}/tmp.txt \
        ${RESULTDIR}/bench_report.txt \
        "${xdim}:${ydim}:${channel}"
    cat ${RESULTDIR}/side_band_info.txt >> ${RESULTDIR}/tf/${network}_${benchmark_type}.${time}
    cat ${RESULTDIR}/bench_report.txt >> ${RESULTDIR}/tf/${network}_${benchmark_type}.${time}
    post_clean
}


#default values
thread_count_default=-1

# $1 holds benchmark type
# =======================
benchmark_type=${1}

# $2 holds metadata
# =================
input_name=$(echo "${2}"| cut -d: -f1)
input_type=$(echo "${2}"| cut -d: -f2)
output_name=$(echo "${2}"| cut -d: -f3)
input_shape=$(echo "${2}"| cut -d: -f4)
network=$(echo "${2}"| cut -d: -f5)

echo "input_name is ${input_name}"
echo "input_type is ${input_type}"
echo "output_name is ${output_name}"
echo "input_shape is ${input_shape}"
echo "network is ${network}"

case "${benchmark_type}" in
(monoshot|core_scale|batch_scale|shape_scale)
    model_file=$(echo "${2}"| cut -d: -f6)
    echo "meta_data is ${2}"
    echo "model file for ${benchmark_type} is ${model_file}"
    ;;
(thread_scale|core_thread_scale)
    thread=$(echo "${2}"| cut -d: -f6)
    model_file=$(echo "${2}"| cut -d: -f7)
    echo "thread count for ${benchmark_type} is ${thread}"
    echo "model file for ${benchmark_type} is ${model_file}"
    ;;
esac


case "${benchmark_type}" in
(batch_scale)
    # $3 holds batch sizes for batch_scale
    # =====================================
    echo "batch size validation should be done by each ML framework"
    echo "batch size is ${3}"
    start=$(echo "${3}"| cut -d: -f1)
    end=$(echo "${3}"| cut -d: -f2)
    step=$(echo "${3}"| cut -d: -f3)
    echo "start is ${start}"
    echo "end is ${end}"
    echo "step is ${step}"
    ;;
(shape_scale)
    # $3 hold auxillary data (shape)
    # ==============================
    # the fields are saved in vars because later when we
    # invoke function for each bench type we don't have
    # to pass args. If we don't save the args in variables
    # then inside the function the positional arguments will
    # all be zero
    batch=$(echo "${input_shape}"| cut -d, -f1)
    xdim=$(echo "${3}"| cut -d: -f1)
    ydim=$(echo "${3}"| cut -d: -f2)
    channel=$(echo "${3}"| cut -d: -f3)
    echo "batch is ${batch}"
    echo "xdim is ${xdim}"
    echo "ydim is ${ydim}"
    echo "channel is ${channel}"
    ;;
esac

# Invoke the right benchmark type
if [ "${benchmark_type}" = "core_scale" ]; then
    echo "Inferencing/s wrt core scaling" 
    run_scaling_wrt_core
elif [ "${benchmark_type}" = "core_thread_scale" ]; then
    echo "Inferencing/s wrt core x thread scaling" 
    run_scaling_wrt_core_thread
elif [ "${benchmark_type}" = "thread_scale" ]; then
    echo "Inferencing/s wrt core x thread scaling" 
    run_scaling_wrt_thread
elif [ "${benchmark_type}" = "batch_scale" ]; then
    echo "Inferencing/s wrt batch scaling" 
    run_scaling_wrt_batch
elif [ "${benchmark_type}" = "shape_scale" ]; then
    echo "Inferencing/s wrt shape scaling" 
    run_scaling_wrt_shape
elif [ "${benchmark_type}" = "monoshot" ]; then
    echo "Inferencing/s monoshot" 
    run_monoshot
else
    echo "Tensorflow : unknown benchmark type selected"
fi

# Move back to parent directory
cd ${WORKDIR}
echo "SUCCESS"
return
